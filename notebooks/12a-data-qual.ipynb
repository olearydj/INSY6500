{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Quality & Validation - Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook demonstrates the implementation of data quality methods using simple toy examples. Each section shows the key Python methods and their usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Part 1: Type Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Converting to Numeric\n",
    "\n",
    "**Method:** `pd.to_numeric(series, errors='coerce')`\n",
    "\n",
    "- `errors='coerce'`: Invalid values become NaN\n",
    "- `errors='raise'`: Throw error on invalid values\n",
    "- `errors='ignore'`: Return original if conversion fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with mixed types\n",
    "data = pd.Series(['25', '30.5', 'unknown', '45', '12.3'])\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(f\"Type: {data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric\n",
    "data_numeric = pd.to_numeric(data, errors='coerce')\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(data_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Converting to Datetime\n",
    "\n",
    "**Method:** `pd.to_datetime(series, errors='coerce')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date strings with an invalid date\n",
    "dates = pd.Series(['2024-01-15', '2024-02-20', 'not-a-date', '2024-03-10'])\n",
    "\n",
    "dates_converted = pd.to_datetime(dates, errors='coerce')\n",
    "print(\"Original:\")\n",
    "print(dates)\n",
    "print(\"\\nConverted:\")\n",
    "print(dates_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Part 2: Range Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Checking Value Ranges\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- Boolean masking: `df[condition]`\n",
    "- `.between(lower, upper)`: Cleaner syntax for ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with invalid values\n",
    "ages = pd.Series([25, 30, -5, 45, 150, 28, 35])\n",
    "print(\"Ages:\", ages.tolist())\n",
    "\n",
    "# Find invalid ages (must be 0-120)\n",
    "invalid = (ages < 0) | (ages > 120)\n",
    "print(f\"\\nInvalid ages: {invalid.sum()}\")\n",
    "print(f\"Invalid values: {ages[invalid].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .between() method\n",
    "valid = ages.between(0, 120)\n",
    "print(f\"Valid ages: {valid.sum()}\")\n",
    "print(f\"Valid values: {ages[valid].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix by setting invalid values to NaN\n",
    "ages_fixed = ages.copy()\n",
    "ages_fixed[~ages.between(0, 120)] = np.nan\n",
    "print(\"\\nFixed ages:\")\n",
    "print(ages_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Part 3: Duplicate Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Finding Duplicates\n",
    "\n",
    "**Method:** `.duplicated(keep='first'/'last'/False)`\n",
    "\n",
    "- `keep='first'`: Mark duplicates as True except first occurrence\n",
    "- `keep='last'`: Mark duplicates as True except last occurrence\n",
    "- `keep=False`: Mark all duplicates including first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with duplicates\n",
    "df = pd.DataFrame({\n",
    "    'id': [1, 2, 2, 3, 3, 3, 4],\n",
    "    'value': ['a', 'b', 'b', 'c', 'c', 'c', 'd']\n",
    "})\n",
    "print(\"Original data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates (default: keep='first')\n",
    "dups = df.duplicated()\n",
    "print(\"\\nDuplicates (keep='first'):\")\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all instances of duplicated rows\n",
    "all_dups = df.duplicated(keep=False)\n",
    "print(\"\\nAll duplicate instances (keep=False):\")\n",
    "print(df[all_dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "**Method:** `.drop_duplicates(subset=None, keep='first')`\n",
    "\n",
    "By default checks all columns for duplicates (`subset=None`) keeping the first instance (same notation as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df_clean = df.drop_duplicates()\n",
    "print(\"After removing duplicates:\")\n",
    "print(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove based on specific columns only\n",
    "df2 = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Alice', 'Charlie'],\n",
    "    'age': [25, 30, 26, 35],\n",
    "    'city': ['NYC', 'LA', 'NYC', 'Chicago']\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df2)\n",
    "\n",
    "# Remove duplicates based on name only\n",
    "df2_clean = df2.drop_duplicates(subset=['name'])\n",
    "print(\"\\nDuplicates removed (based on 'name'):\")\n",
    "print(df2_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Part 4: Detecting Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Identifying Missing Values\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- `.isna()` / `.isnull()`: Returns Boolean Series (True = missing)\n",
    "- `.notna()` / `.notnull()`: Returns Boolean Series (True = not missing)\n",
    "\n",
    "`isna` and `isnull` are equivalent, as are `notna` and `notnull`, but the `na` forms are strongly preferred now. Use them. The `null` forms are included here as they show up in older code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with missing values\n",
    "data = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, np.nan, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "print(\"Data:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values (True = missing):\")\n",
    "print(data.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "print(\"\\nMissing count by column:\")\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per row\n",
    "print(\"\\nMissing count by row:\")\n",
    "print(data.isna().sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Part 5: Handling Missing Data - Drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Dropping Missing Values\n",
    "\n",
    "**Method:** `.dropna(axis=0, how='any', subset=None, thresh=None)`\n",
    "\n",
    "- `axis=0`: Drop rows; `axis=1`: Drop columns\n",
    "- `how='any'`: Drop if any value is missing\n",
    "- `how='all'`: Drop only if all values are missing\n",
    "- `subset`: Check only specific columns\n",
    "- `thresh=N`: Require at least N non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, np.nan, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(f\"Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entire rows where any value is missing\n",
    "dropped_any = data.dropna()\n",
    "print(\"\\nAfter dropna() - any missing:\")\n",
    "print(dropped_any)\n",
    "print(f\"Shape: {dropped_any.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only if specific column is missing\n",
    "dropped_subset = data.dropna(subset=['A'])\n",
    "print(\"\\nAfter dropna(subset=['A']):\")\n",
    "print(dropped_subset)\n",
    "print(f\"Shape: {dropped_subset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only if at least N values are non-null\n",
    "dropped_thresh = data.dropna(thresh=2)\n",
    "print(\"\\nAfter dropna(thresh=2) - keep rows with at least 2 non-null:\")\n",
    "print(dropped_thresh)\n",
    "print(f\"Shape: {dropped_thresh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Part 6: Handling Missing Data - Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Filling Missing Values\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- `.fillna(value)` - replace missing with specified value\n",
    "  - `value`: Constant, Series, DataFrame, or dict specifying replacements for each column\n",
    "- `.ffill()` - forward fille (use previous value)\n",
    "- `.bfill()`- backward fill (use next value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 3, np.nan, 5])\n",
    "print(\"Original:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with a constant\n",
    "filled_constant = data.fillna(0)\n",
    "print(\"\\nFilled with 0:\")\n",
    "print(filled_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with mean\n",
    "filled_mean = data.fillna(data.mean())\n",
    "print(\"\\nFilled with mean:\")\n",
    "print(filled_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with different values per column using a dict\n",
    "data_df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [10, np.nan, np.nan, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "filled_dict = data_df.fillna({'A': 0, 'B': 999})\n",
    "print(\"\\nOriginal:\")\n",
    "print(data_df)\n",
    "print(\"\\nFilled with dict (A=0, B=999):\")\n",
    "print(filled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill\n",
    "filled_ffill = data.ffill()\n",
    "print(\"\\nForward fill:\")\n",
    "print(filled_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Group-Based Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with groups\n",
    "df = pd.DataFrame({\n",
    "    'group': ['A', 'A', 'A', 'B', 'B', 'B'],\n",
    "    'value': [10, np.nan, 12, 20, np.nan, 22]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with group-specific mean\n",
    "df['value_filled'] = df.groupby('group')['value'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "print(\"\\nFilled with group means:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Part 7: Handling Missing Data - Interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "**Method:** `.interpolate(method='linear')`\n",
    "\n",
    "- `method='linear'`: Straight line between points (default)\n",
    "- `method='time'`: Uses actual time intervals\n",
    "- Only use for ordered data (time series, spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series with gaps\n",
    "series = pd.Series([1, np.nan, np.nan, 4, 5, np.nan, 7])\n",
    "print(\"Original:\")\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation\n",
    "interpolated = series.interpolate()\n",
    "print(\"\\nAfter interpolation:\")\n",
    "print(interpolated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interpolation\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "series.plot(ax=ax, style='o-', label='Original (with gaps)', markersize=8, linewidth=2, alpha=0.6)\n",
    "interpolated.plot(ax=ax, style='s-', label='Interpolated', markersize=6, linewidth=2)\n",
    "ax.set_title('Linear Interpolation')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Part 8: Outlier Detection - IQR Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### IQR Method\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "- IQR = Q3 - Q1\n",
    "- Lower bound = Q1 - 1.5 × IQR\n",
    "- Upper bound = Q3 + 1.5 × IQR\n",
    "\n",
    "**Method:** `.quantile(q)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with outliers\n",
    "data = pd.Series([10, 12, 13, 14, 15, 16, 18, 20, 22, 100])\n",
    "print(\"Data:\", data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Q1: {Q1}\")\n",
    "print(f\"Q3: {Q3}\")\n",
    "print(f\"IQR: {IQR}\")\n",
    "print(f\"\\nBounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers\n",
    "outliers = (data < lower_bound) | (data > upper_bound)\n",
    "print(f\"\\nOutliers detected: {outliers.sum()}\")\n",
    "print(f\"Outlier values: {data[outliers].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Part 9: Outlier Detection - Z-Score Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Z-Score Method\n",
    "\n",
    "**Formula:** z = (x - mean) / std\n",
    "\n",
    "**Rule:** |z| > 3 indicates outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data\n",
    "data = pd.Series([10, 12, 13, 14, 15, 16, 18, 20, 22, 100])\n",
    "\n",
    "# Calculate z-scores\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "z_scores = (data - mean) / std\n",
    "\n",
    "print(\"Data:\", data.tolist())\n",
    "print(f\"\\nMean: {mean:.2f}, Std: {std:.2f}\")\n",
    "print(\"\\nZ-scores:\", z_scores.round(2).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers (|z| > 3)\n",
    "outliers_z = np.abs(z_scores) > 3\n",
    "print(f\"\\nOutliers (|z| > 3): {outliers_z.sum()}\")\n",
    "print(f\"Outlier values: {data[outliers_z].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Part 10: Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Three Approaches\n",
    "\n",
    "1. Remove outliers\n",
    "2. Cap outliers (winsorize)\n",
    "3. Keep and flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([10, 12, 13, 14, 15, 16, 18, 20, 22, 100])\n",
    "\n",
    "# Calculate bounds (using IQR)\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Original data: {data.tolist()}\")\n",
    "print(f\"Upper bound: {upper:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Remove outliers\n",
    "data_removed = data[data <= upper].copy()\n",
    "print(f\"\\nRemoved outliers: {data_removed.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Cap outliers\n",
    "data_capped = data.clip(upper=upper)\n",
    "print(f\"\\nCapped at {upper:.2f}: {data_capped.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3: Flag outliers\n",
    "df_flagged = pd.DataFrame({\n",
    "    'value': data,\n",
    "    'is_outlier': data > upper\n",
    "})\n",
    "print(\"\\nFlagged outliers:\")\n",
    "print(df_flagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Part 11: End-to-End Example with Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Now let's apply these methods to real data. We'll use the Titanic dataset as-is and perform a systematic data quality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "print(f\"Dataset shape: {titanic.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check data types\n",
    "print(\"Data types:\")\n",
    "print(titanic.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check for missing values\n",
    "missing = titanic.isna().sum()\n",
    "missing_pct = (missing / len(titanic) * 100).round(1)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': missing_pct[missing > 0]\n",
    "}).sort_values('Missing', ascending=False)\n",
    "\n",
    "print(\"\\nMissing data:\")\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check for duplicates\n",
    "dups = titanic.duplicated()\n",
    "print(f\"\\nDuplicate rows: {dups.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Check value ranges\n",
    "print(\"\\nAge statistics:\")\n",
    "print(titanic['age'].describe())\n",
    "\n",
    "print(\"\\nPassenger class values:\")\n",
    "print(titanic['pclass'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nFare statistics:\")\n",
    "print(titanic['fare'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Identify outliers in fare (using IQR)\n",
    "Q1 = titanic['fare'].quantile(0.25)\n",
    "Q3 = titanic['fare'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "fare_outliers = (titanic['fare'] < lower) | (titanic['fare'] > upper)\n",
    "print(f\"\\nFare outliers: {fare_outliers.sum()} ({fare_outliers.sum()/len(titanic)*100:.1f}%)\")\n",
    "print(f\"Upper bound: ${upper:.2f}\")\n",
    "print(f\"Max fare: ${titanic['fare'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Make cleaning decisions\n",
    "\n",
    "# Decision 1: Drop rows where 'survived' is missing (critical variable)\n",
    "titanic_clean = titanic.dropna(subset=['survived']).copy()\n",
    "print(f\"After dropping rows with missing 'survived': {len(titanic_clean)} rows\")\n",
    "\n",
    "# Decision 2: Fill missing 'age' with median by passenger class (MAR pattern)\n",
    "titanic_clean['age'] = titanic_clean.groupby('pclass')['age'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# Decision 3: Fill missing 'embarked' with mode\n",
    "mode_embarked = titanic_clean['embarked'].mode()[0]\n",
    "titanic_clean['embarked'] = titanic_clean['embarked'].fillna(mode_embarked)\n",
    "\n",
    "# Decision 4: Keep fare outliers (legitimate high fares exist)\n",
    "print(f\"\\nFinal dataset: {len(titanic_clean)} rows\")\n",
    "print(f\"Remaining missing values:\\n{titanic_clean.isna().sum().sum()} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final quality check\n",
    "print(\"Final Data Quality Report:\")\n",
    "print(f\"Shape: {titanic_clean.shape}\")\n",
    "print(f\"\\nMissing values by column:\")\n",
    "print(titanic_clean.isna().sum())\n",
    "print(f\"\\nDuplicates: {titanic_clean.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key methods learned:**\n",
    "\n",
    "| Task | Method | Key Parameter |\n",
    "|------|--------|---------------|\n",
    "| Convert to numeric | `pd.to_numeric()` | `errors='coerce'` |\n",
    "| Check range | `.between()` | `lower`, `upper` |\n",
    "| Find duplicates | `.duplicated()` | `keep='first'/'last'/False` |\n",
    "| Remove duplicates | `.drop_duplicates()` | `subset=['col']` |\n",
    "| Drop missing | `.dropna()` | `subset=['col']`, `thresh=N` |\n",
    "| Fill missing | `.fillna()` | `value`, `method='ffill'` |\n",
    "| Interpolate | `.interpolate()` | `method='linear'` |\n",
    "| Get quantiles | `.quantile()` | `q=0.25` |\n",
    "| Cap values | `.clip()` | `lower`, `upper` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "INSY6500-Py4EDA",
   "language": "python",
   "name": "insy6500-py4eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
