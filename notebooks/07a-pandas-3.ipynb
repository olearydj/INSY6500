{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Working with Real Data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "So far we've worked with small, hand-crafted datasets to learn Pandas fundamentals. In practice, you'll load data from files, explore it to understand its structure and quality, manipulate it to suit your analysis needs, and save the results.\n",
    "\n",
    "This notebook covers the essential skills for that workflow:\n",
    "\n",
    "- Reading data from CSV files (and other formats)\n",
    "- Exploring and diagnosing loaded data\n",
    "- Setting and resetting index structures\n",
    "- Filtering data with boolean indexing and queries\n",
    "- Creating custom transformations with `apply()`\n",
    "- Working with dates and times\n",
    "- Saving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Loading Data from Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The most common way to get data into Pandas is from CSV (comma-separated values) files. The `read_csv()` function handles this with many options to control how the data is loaded and interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Basic CSV Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "At its simplest, `read_csv()` takes a file path and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll create a sample CSV file first\n",
    "# In practice, you'll usually load existing files\n",
    "\n",
    "sample_data = \"\"\"date,product,quantity,price,region\n",
    "2024-01-15,Widget A,100,25.50,North\n",
    "2024-01-16,Widget B,150,30.00,South\n",
    "2024-01-17,Widget A,200,25.50,East\n",
    "2024-01-18,Widget C,75,45.00,West\n",
    "2024-01-19,Widget B,125,30.00,North\n",
    "2024-01-20,Widget A,,,South\n",
    "2024-01-21,Widget C,90,45.00,East\n",
    "\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open('data/07a_sales_sample.csv', 'w') as f:\n",
    "    f.write(sample_data)\n",
    "\n",
    "# Load it back\n",
    "df = pd.read_csv('data/07a_sales_sample.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Notice that Pandas automatically:\n",
    "- Used the first row as column names\n",
    "- Assigned a numeric index starting at 0\n",
    "- Inferred data types for each column (minimal, strings and floats)\n",
    "- Recognized empty values as `NaN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Initial Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Before diving into analysis, we need to understand what we're working with. Pandas provides several tools for quick data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The `info()` method shows:\n",
    "- Number of rows and columns\n",
    "- Column names and data types\n",
    "- Non-null counts (helps identify missing data)\n",
    "- Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types explicitly\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The `describe()` method provides count, mean, standard deviation, min, quartiles, and max for each numeric column. This is useful for spotting outliers and understanding data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in a column\n",
    "print(\"Unique products:\")\n",
    "print(df['product'].unique())\n",
    "\n",
    "print(\"\\nNumber of unique products:\")\n",
    "print(df['product'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each value\n",
    "print(\"Product value counts:\")\n",
    "print(df['product'].value_counts())\n",
    "\n",
    "print(\"\\nRegion value counts:\")\n",
    "print(df['region'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The `value_counts()` method is particularly useful for understanding the distribution of categorical data. It returns a Series with counts sorted in descending order by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Controlling Data Types on Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Notice that the `date` column was loaded as an object (string) rather than an object type that represents dates. We can control how columns are interpreted using parameters to `read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data types and parse dates\n",
    "df = pd.read_csv('data/07a_sales_sample.csv',\n",
    "                 dtype={'product': 'string', 'region': 'string'},\n",
    "                 parse_dates=['date'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Now the `date` column is properly recognized as a datetime64 type, which enables time-based operations we'll explore later. For now we'll just look at the internal representation:\n",
    "\n",
    "djo: why is this a Timestamp object and not datetime? it there a more clearcut way to show this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'date'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Handling Missing Values on Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "You can specify additional values that should be treated as missing (beyond the defaults like empty strings and 'NA')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file with various missing value representations\n",
    "messy_data = \"\"\"name,score,status\n",
    "Alice,95,active\n",
    "Bob,N/A,inactive\n",
    "Charlie,88,active\n",
    "Diana,unknown,active\n",
    "Eve,92,n/a\n",
    "\"\"\"\n",
    "\n",
    "with open('data/07a_messy_sample.csv', 'w') as f:\n",
    "    f.write(messy_data)\n",
    "\n",
    "# Load with custom missing value indicators\n",
    "df_messy = pd.read_csv('data/07a_messy_sample.csv',\n",
    "                       na_values=['N/A', 'unknown', 'n/a'])\n",
    "\n",
    "print(df_messy)\n",
    "print(\"\\nNull value counts:\")\n",
    "print(df_messy.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Setting the Index on Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Often, one column naturally serves as an index (like dates or IDs). You can set this during loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use date as the index\n",
    "df_indexed = pd.read_csv('data/07a_sales_sample.csv',\n",
    "                         parse_dates=['date'],\n",
    "                         index_col='date')\n",
    "\n",
    "df_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "This can be more efficient than loading and then setting the index separately, especially with large files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Memory Diagnostics for Larger Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "When working with larger datasets, memory usage becomes important. The `memory_usage()` method helps identify which columns consume the most memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger sample dataset\n",
    "np.random.seed(42)\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(10000),\n",
    "    'value': np.random.randn(10000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 10000),\n",
    "    'date': pd.date_range('2020-01-01', periods=10000, freq='h'),\n",
    "    'description': ['Item ' + str(i) for i in range(10000)]\n",
    "})\n",
    "\n",
    "# Check memory usage by column\n",
    "print(\"Memory usage by column:\")\n",
    "print(large_df.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nTotal memory usage:\")\n",
    "print(f\"{large_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The `deep=True` parameter includes the actual memory used by object types like strings, which can be significant. Strings often represent categorical data, for which Pandas has a specific data type. By converting the `category` column to a categorical type we save memory *and* enable valuable functionality for that type (explored later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize by converting category column to categorical dtype\n",
    "large_df['category'] = large_df['category'].astype('category')\n",
    "\n",
    "print(\"Memory usage after optimization:\")\n",
    "print(large_df.memory_usage(deep=True))\n",
    "\n",
    "print(\"\\nTotal memory usage:\")\n",
    "print(f\"{large_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Converting columns with repeated values to the `category` dtype can dramatically reduce memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Writing Data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "After processing data, you'll often need to save the results. The `to_csv()` method handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV (default: includes index)\n",
    "df.to_csv('data/07a_sales_processed.csv')\n",
    "\n",
    "# Verify what was written\n",
    "with open('data/07a_sales_processed.csv', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content[:200], '...')  # First 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Note: The leading comma in the header is expected - it's an empty column header for the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save without index (common for data without meaningful index)\n",
    "df.to_csv('data/07a_sales_no_index.csv', index=False)\n",
    "\n",
    "with open('data/07a_sales_no_index.csv', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only specific columns\n",
    "df.to_csv('data/07a_sales_subset.csv', \n",
    "          columns=['date', 'product', 'quantity'],\n",
    "          index=False)\n",
    "\n",
    "# Verify\n",
    "pd.read_csv('data/07a_sales_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Pandas' `read_csv` has many other options..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Other File Formats\n",
    "\n",
    "Pandas supports many file formats beyond CSV. Excel files are particularly common in business settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Excel (requires openpyxl or xlsxwriter package)\n",
    "try:\n",
    "    df.to_excel('data/07a_sales.xlsx', sheet_name='Sales', index=False)\n",
    "    print(\"Excel file written successfully\")\n",
    "    \n",
    "    # Read it back\n",
    "    df_excel = pd.read_excel('data/07a_sales.xlsx', sheet_name='Sales')\n",
    "    print(\"\\nLoaded from Excel:\")\n",
    "    print(df_excel.head())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Excel support requires: pip install openpyxl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "For large datasets, CSV is generally faster and more reliable than Excel.\n",
    "\n",
    "**Note:** Excel operations in Pandas require additional packages. Though Pandas relies on `openpyxl` for this, it is not identified as a default dependency, so is not added by conda during `conda install pandas`. Use `conda install openpyxl` in your `insy6500` environment if required. You do not need to import `openpyxl` - that is handled by Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Index Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "The index is more than just row numbers—it's a powerful tool for data organization and access. Understanding when and how to set an index is key to efficient Pandas use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Setting an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "The `set_index()` method converts a column into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with default numeric index\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "print(\"\\nIndex:\", df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date as index\n",
    "df_dated = df.set_index('date')\n",
    "print(df_dated)\n",
    "print(\"\\nIndex:\", df_dated.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Remember that most Pandas methods return a new DataFrame rather than modifying the original. To update the original, either reassign it or use `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Reassign\n",
    "df = df.set_index('date')\n",
    "\n",
    "# Option 2: In-place (equivalent to above)\n",
    "# df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### When to Use an Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Setting a meaningful index is beneficial when:\n",
    "\n",
    "1. **Time series data**: Date/time indexes enable powerful time-based operations\n",
    "2. **Unique identifiers**: Customer IDs, product codes, etc.\n",
    "3. **Frequent lookups**: If you often access rows by a specific value\n",
    "4. **Merging data**: Indexes are used in join operations\n",
    "\n",
    "Keep the default numeric index when:\n",
    "- No column naturally serves as an identifier\n",
    "- You primarily work with all rows at once\n",
    "- The data structure may change frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Resetting the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "The `reset_index()` method converts the index back into a regular column and creates a new default numeric index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current state: date is the index\n",
    "print(\"Before reset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Reset to default numeric index\n",
    "df_reset = df.reset_index()\n",
    "print(\"\\nAfter reset:\")\n",
    "print(df_reset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### The `drop` Parameter\n",
    "\n",
    "When resetting an index, you can choose whether to keep the old index as a column or discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and drop the old index\n",
    "df_dropped = df.reset_index(drop=True)\n",
    "print(\"After reset with drop=True:\")\n",
    "print(df_dropped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "This is useful when the index was created for temporary operations and isn't needed in the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Common Index Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Here are some typical scenarios where index operations are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Fixing data structure after aggregation\n",
    "# groupby() splits data into groups and applies aggregation functions\n",
    "# Here we group by product and sum the quantities\n",
    "# The grouping column (product) automatically becomes the index\n",
    "grouped = df.groupby('product')['quantity'].sum()\n",
    "print(\"Grouped result (product becomes index):\")\n",
    "print(grouped)\n",
    "\n",
    "# Reset to make product a column again - useful for further analysis or output\n",
    "grouped_df = grouped.reset_index()\n",
    "print(\"\\nAfter reset (product is now a column):\")\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Preparing for time series analysis\n",
    "# Create sample time series data\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=7, freq='D'),\n",
    "    'sales': [100, 120, 115, 130, 125, 140, 135],\n",
    "    'costs': [70, 85, 80, 90, 88, 95, 92]\n",
    "})\n",
    "\n",
    "print(\"Before setting date index:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Set date as index for time series operations\n",
    "ts_data = ts_data.set_index('date')\n",
    "print(\"\\nAfter setting date index:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Now we can use date-based selection\n",
    "print(\"\\nSales for first 3 days:\")\n",
    "print(ts_data.loc['2024-01-01':'2024-01-03'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "**Note on date slicing:** The `.loc['2024-01-01':'2024-01-03']` syntax works here because `date` is the index *and* is a `datetime64` type. If `date` were just a regular column (not the index), you'd need to use boolean indexing instead: `ts_data[ts_data['date'].between('2024-01-01', '2024-01-03')]`. The datetime index enables this convenient slicing syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Boolean Indexing and Queries\n",
    "\n",
    "We've seen basic boolean indexing before. Now we'll explore more complex filtering patterns that are essential for real data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our working DataFrame for these examples\n",
    "df = pd.read_csv('data/07a_sales_sample.csv', parse_dates=['date'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### Multiple Conditions with Boolean Indexing\n",
    "\n",
    "Complex filters require combining conditions with `&` (and) or `|` (or). Each condition must be wrapped in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-value sales (quantity > 100 AND price > 30)\n",
    "high_value = df[(df['quantity'] > 100) & (df['price'] > 30)]\n",
    "print(\"High value sales:\")\n",
    "print(high_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "The empty result is expected - our small sample dataset doesn't have any sales with *both* quantity > 100 *and* price > 30. Widget B has the right price (30.0) but we need strictly greater than 30. This illustrates how multiple conditions can filter data down to nothing if they're too restrictive. Let's try a more lenient version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More useful: Find high-value sales (quantity > 100 AND price >= 30)\n",
    "high_value = df[(df['quantity'] > 100) & (df['price'] >= 30)]\n",
    "print(\"High value sales (quantity > 100 AND price >= 30):\")\n",
    "print(high_value)\n",
    "\n",
    "# OR conditions: Find Widget A OR Widget C sales\n",
    "widget_ac = df[(df['product'] == 'Widget A') | (df['product'] == 'Widget C')]\n",
    "print(\"\\nWidget A or C sales:\")\n",
    "print(widget_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex: Widget A in North OR South regions with quantity > 100\n",
    "complex_filter = df[\n",
    "    (df['product'] == 'Widget A') & \n",
    "    (df['region'].isin(['North', 'South'])) &\n",
    "    (df['quantity'] > 100)\n",
    "]\n",
    "print(\"Complex filter result:\")\n",
    "print(complex_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Again, an empty result - a good lesson about **over-specification**. We're asking for Widget A, in North or South regions, with quantity > 100. Our Widget A sales are:\n",
    "- 100 units in North (fails the quantity test)\n",
    "- 200 units in East (fails the region test)\n",
    "- NaN in South (fails the quantity test)\n",
    "\n",
    "When debugging empty filter results, relax constraints one at a time to see which condition is eliminating your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "**Important:** Use `&` and `|` for element-wise boolean operations in Pandas, not `and` and `or` (which are for Python boolean values only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### The `isin()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "As demonstrated above, when checking if values are in a list of options, `isin()` is cleaner than multiple `|` conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of: (df['region'] == 'North') | (df['region'] == 'South') | (df['region'] == 'East')\n",
    "# Use isin:\n",
    "selected_regions = df[df['region'].isin(['North', 'South', 'East'])]\n",
    "print(\"Sales in North, South, or East:\")\n",
    "print(selected_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also use negation with ~\n",
    "not_west = df[~df['region'].isin(['West'])]\n",
    "print(\"\\nSales NOT in West:\")\n",
    "print(not_west)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### The `query()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "For complex filters, `query()` provides a more readable alternative to boolean indexing. It accepts a string expression and evaluates it against the DataFrame.\n",
    "\n",
    "The string is interpreted as a Python expression where column names become variables. So instead of writing `df[(df['quantity'] > 100)]`, you can write `df.query('quantity > 100')`. This is especially helpful for complex multi-condition filters that would otherwise require many parentheses and ampersands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing version\n",
    "result1 = df[(df['quantity'] > 100) & (df['price'] < 30)]\n",
    "\n",
    "# query() version - more readable!\n",
    "result2 = df.query('quantity > 100 and price < 30')\n",
    "\n",
    "print(\"Using query():\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query() with string matching\n",
    "widget_b_sales = df.query('product == \"Widget B\"')\n",
    "print(\"Widget B sales:\")\n",
    "print(widget_b_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "**Using variables in queries:** The `@` symbol tells `query()` to look for a variable in the surrounding Python scope rather than treating it as a column name. Without `@`, `query('quantity >= min_quantity')` would look for a column called `min_quantity` in the DataFrame (and fail). With `@min_quantity`, it uses the Python variable's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query() can reference variables with @\n",
    "min_quantity = 100\n",
    "target_region = 'North'\n",
    "\n",
    "result = df.query('quantity >= @min_quantity and region == @target_region')\n",
    "print(f\"Sales >= {min_quantity} in {target_region}:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Combining `query()` with `loc[]` for Filtered Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "You can combine filtering with assignment to modify specific subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to demonstrate\n",
    "df_modified = df.copy()\n",
    "\n",
    "# Add a discount column\n",
    "df_modified['discount'] = 0.0\n",
    "\n",
    "# Apply 10% discount to high-quantity orders using boolean indexing\n",
    "high_qty_mask = df_modified['quantity'] > 100\n",
    "df_modified.loc[high_qty_mask, 'discount'] = 0.10\n",
    "\n",
    "print(\"After applying discounts:\")\n",
    "print(df_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex: apply different discounts based on multiple conditions\n",
    "df_modified = df.copy()\n",
    "df_modified['discount'] = 0.0\n",
    "\n",
    "# 15% discount for large orders of Widget A\n",
    "mask1 = (df_modified['product'] == 'Widget A') & (df_modified['quantity'] > 150)\n",
    "df_modified.loc[mask1, 'discount'] = 0.15\n",
    "\n",
    "# 10% discount for all other large orders\n",
    "mask2 = (df_modified['quantity'] > 100) & (df_modified['discount'] == 0)\n",
    "df_modified.loc[mask2, 'discount'] = 0.10\n",
    "\n",
    "print(\"After tiered discounts:\")\n",
    "print(df_modified[['product', 'quantity', 'price', 'discount']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "**Note:** When making filtered assignments, always use `.loc[]` to avoid the \"SettingWithCopyWarning\" that can indicate subtle bugs. This aligns with previous guidance for when to use various subsetting methods in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## Custom Transformations with `apply()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "While vectorized operations (like `df['price'] * 1.1`) are fast and should be used when possible, sometimes you need custom logic that doesn't fit a simple formula. That's where `apply()` comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### Basic `apply()` with Lambda Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "The `apply()` method takes a function and applies it to each element (or row/column) of a Series or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple transformation: categorize quantities\n",
    "df['quantity_category'] = df['quantity'].apply(\n",
    "    lambda x: 'High' if x > 150 else 'Medium' if x > 100 else 'Low'\n",
    ")\n",
    "\n",
    "print(df[['quantity', 'quantity_category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String manipulation: extract first word from product name\n",
    "df['product_type'] = df['product'].apply(lambda x: x.split()[0])\n",
    "\n",
    "print(df[['product', 'product_type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "### When to Use `apply()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "Use `apply()` when:\n",
    "- The operation requires conditional logic (if/else)\n",
    "- You need to call functions that aren't vectorized\n",
    "- The transformation depends on multiple conditions\n",
    "\n",
    "Avoid `apply()` when:\n",
    "- A vectorized operation exists (e.g., `df['A'] + df['B']`)\n",
    "- A built-in Pandas method handles your case\n",
    "\n",
    "Vectorized operations are much faster than `apply()` for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Vectorized approach is better when possible\n",
    "\n",
    "# SLOW (using apply)\n",
    "# df['total'] = df.apply(lambda row: row['quantity'] * row['price'], axis=1)\n",
    "\n",
    "# FAST (vectorized)\n",
    "df['total'] = df['quantity'] * df['price']\n",
    "\n",
    "print(df[['quantity', 'price', 'total']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "### Applying Functions to Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "By default, `apply()` works on columns. Use `axis=1` to work with rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that uses multiple columns\n",
    "def calculate_status(row):\n",
    "    if pd.isna(row['quantity']) or pd.isna(row['price']):\n",
    "        return 'Incomplete'\n",
    "    elif row['quantity'] * row['price'] > 5000:\n",
    "        return 'High Value'\n",
    "    elif row['quantity'] > 150:\n",
    "        return 'High Volume'\n",
    "    else:\n",
    "        return 'Standard'\n",
    "\n",
    "df['order_status'] = df.apply(calculate_status, axis=1)\n",
    "\n",
    "print(df[['quantity', 'price', 'order_status']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "**Note:** Applying functions row-wise (`axis=1`) is generally slower than column-wise operations. When performance matters with large datasets, consider vectorized alternatives or NumPy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## Working with Dates and Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "Time series data is ubiquitous in data analysis. Pandas provides excellent support for dates and times, which we'll explore in depth in a future lesson (12a). For now, we'll cover the basics you need for most analyses.\n",
    "\n",
    "### Converting to Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "We've seen the `parse_dates` parameter in `read_csv()`. You can also convert existing columns with `pd.to_datetime()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with date strings\n",
    "date_data = pd.DataFrame({\n",
    "    'date_str': ['2024-01-15', '2024-02-20', '2024-03-10'],\n",
    "    'value': [100, 200, 150]\n",
    "})\n",
    "\n",
    "print(\"Before conversion:\")\n",
    "print(date_data.dtypes)\n",
    "\n",
    "# Convert to datetime\n",
    "date_data['date'] = pd.to_datetime(date_data['date_str'])\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(date_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime() can handle various formats with format='mixed'\n",
    "messy_dates = pd.Series([\n",
    "    '2024-01-15',\n",
    "    'January 15, 2024',\n",
    "    '01/15/2024',\n",
    "    '15-Jan-2024'\n",
    "])\n",
    "\n",
    "# Use format='mixed' to handle multiple date formats\n",
    "clean_dates = pd.to_datetime(messy_dates, format='mixed')\n",
    "print(\"Cleaned dates:\")\n",
    "print(clean_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### The `dt` Accessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "Once a column is datetime type, the `.dt` accessor provides many useful properties and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our sales data with dates\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract components\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "\n",
    "print(df[['date', 'year', 'month', 'day', 'day_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for grouping by time periods\n",
    "by_day = df.groupby(df['date'].dt.day_name())['quantity'].mean()\n",
    "print(\"Average quantity by day of week:\")\n",
    "print(by_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "### Common Datetime Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by date range\n",
    "jan_2024 = df[df['date'].dt.month == 1]\n",
    "print(\"January 2024 sales:\")\n",
    "print(jan_2024[['date', 'product', 'quantity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find weekend sales (Saturday=5, Sunday=6)\n",
    "df['is_weekend'] = df['date'].dt.dayofweek >= 5\n",
    "weekend_sales = df[df['is_weekend']]\n",
    "\n",
    "print(\"Weekend sales:\")\n",
    "print(weekend_sales[['date', 'day_name', 'quantity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time differences\n",
    "df_sorted = df.sort_values('date')\n",
    "df_sorted['days_since_prev'] = df_sorted['date'].diff().dt.days\n",
    "\n",
    "print(\"Days between sales:\")\n",
    "print(df_sorted[['date', 'days_since_prev']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "**Preview of 12a:** We'll cover time series in much more detail later, including resampling (aggregating by week/month), rolling windows (moving averages), time zones, and more advanced datetime operations. For now, these basics will handle most common needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "## Putting It All Together: A Complete Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "Let's combine everything we've learned in a realistic data processing workflow:\n",
    "\n",
    "1. Load\n",
    "2. Explore\n",
    "3. Clean\n",
    "4. Feature Engineering\n",
    "5. Filter and Analyze\n",
    "6. Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more realistic sample dataset\n",
    "extended_data = \"\"\"date,product,quantity,price,region,customer_type\n",
    "2024-01-15,Widget A,100,25.50,North,retail\n",
    "2024-01-16,Widget B,150,30.00,South,wholesale\n",
    "2024-01-17,Widget A,200,25.50,East,retail\n",
    "2024-01-18,Widget C,75,45.00,West,retail\n",
    "2024-01-19,Widget B,125,30.00,North,wholesale\n",
    "2024-01-20,Widget A,,,South,retail\n",
    "2024-01-21,Widget C,90,45.00,East,wholesale\n",
    "2024-01-22,Widget A,175,25.50,North,retail\n",
    "2024-01-23,Widget B,200,30.00,West,wholesale\n",
    "2024-01-24,Widget C,50,45.00,South,retail\n",
    "\"\"\"\n",
    "\n",
    "with open('data/07a_extended_sales.csv', 'w') as f:\n",
    "    f.write(extended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data with appropriate types\n",
    "sales = pd.read_csv('data/07a_extended_sales.csv',\n",
    "                    parse_dates=['date'],\n",
    "                    dtype={'region': 'category', 'customer_type': 'category'})\n",
    "\n",
    "print(\"Step 1 - Loaded data:\")\n",
    "print(sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Explore the data\n",
    "print(\"\\nStep 2 - Data exploration:\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sales.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(sales.describe())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(sales.isna().sum())\n",
    "\n",
    "print(\"\\nValue counts by region:\")\n",
    "print(sales['region'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean the data\n",
    "print(\"\\nStep 3 - Cleaning:\")\n",
    "\n",
    "# Drop rows with missing critical data\n",
    "# The 'subset' parameter specifies which columns to check for NaN\n",
    "# A row is dropped only if it has NaN in ANY of these specified columns\n",
    "sales_clean = sales.dropna(subset=['quantity', 'price'])\n",
    "print(f\"Removed {len(sales) - len(sales_clean)} rows with missing data\")\n",
    "\n",
    "# Set date as index for time-based operations\n",
    "sales_clean = sales_clean.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create derived columns\n",
    "print(\"\\nStep 4 - Feature engineering:\")\n",
    "\n",
    "# Calculate revenue\n",
    "sales_clean['revenue'] = sales_clean['quantity'] * sales_clean['price']\n",
    "\n",
    "# Add day of week\n",
    "sales_clean['day_of_week'] = sales_clean.index.day_name()\n",
    "\n",
    "# Categorize order size\n",
    "sales_clean['order_size'] = sales_clean['quantity'].apply(\n",
    "    lambda x: 'Large' if x >= 175 else 'Medium' if x >= 100 else 'Small'\n",
    ")\n",
    "\n",
    "print(sales_clean[['quantity', 'price', 'revenue', 'order_size']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Filter and analyze\n",
    "print(\"\\nStep 5 - Analysis:\")\n",
    "\n",
    "# High-value wholesale orders\n",
    "high_value_wholesale = sales_clean.query(\n",
    "    'customer_type == \"wholesale\" and revenue > 5000'\n",
    ")\n",
    "print(\"\\nHigh-value wholesale orders:\")\n",
    "print(high_value_wholesale[['product', 'region', 'revenue']])\n",
    "\n",
    "# Average revenue by customer type and region\n",
    "# observed=True prevents warning about future behavior change with categorical data\n",
    "avg_revenue = sales_clean.groupby(['customer_type', 'region'], observed=True)['revenue'].mean()\n",
    "print(\"\\nAverage revenue by customer type and region:\")\n",
    "print(avg_revenue.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save processed data\n",
    "print(\"\\nStep 6 - Saving results:\")\n",
    "\n",
    "# Save full processed dataset\n",
    "sales_clean.to_csv('data/07a_sales_processed_final.csv')\n",
    "print(\"Saved processed data to 07a_sales_processed_final.csv\")\n",
    "\n",
    "# Save summary report\n",
    "summary = sales_clean.groupby(['product', 'customer_type'], observed=True).agg({\n",
    "    'quantity': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'region': 'count'\n",
    "}).rename(columns={'region': 'num_orders'})\n",
    "\n",
    "summary.to_csv('data/07a_sales_summary.csv')\n",
    "print(\"Saved summary report to 07a_sales_summary.csv\")\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered the essential skills for working with real data in Pandas:\n",
    "\n",
    "**File I/O:**\n",
    "- Loading CSV files with `read_csv()` and controlling data types, date parsing, and missing values\n",
    "- Using `info()`, `describe()`, `value_counts()`, and `memory_usage()` to understand loaded data\n",
    "- Saving data with `to_csv()` and basic Excel operations\n",
    "\n",
    "**Index Operations:**\n",
    "- Setting meaningful indexes with `set_index()`\n",
    "- Resetting to default numeric indexes with `reset_index()`\n",
    "- Understanding when to use a custom index vs. default numeric index\n",
    "\n",
    "**Boolean Indexing:**\n",
    "- Filtering with complex conditions using `&`, `|`, and `~`\n",
    "- Using `isin()` for cleaner membership tests\n",
    "- Writing readable filters with `query()`\n",
    "- Combining filters with `loc[]` for conditional assignment\n",
    "\n",
    "**Custom Transformations:**\n",
    "- Using `apply()` with lambda functions for custom logic\n",
    "- Understanding when vectorized operations are better than `apply()`\n",
    "- Applying functions row-wise vs. column-wise\n",
    "\n",
    "**DateTime Basics:**\n",
    "- Converting strings to datetime with `pd.to_datetime()`\n",
    "- Extracting date components with the `.dt` accessor\n",
    "- Filtering and grouping by date attributes\n",
    "\n",
    "These skills form the foundation for most data analysis workflows. You can now load real datasets, explore their structure, clean and transform them, and save the results—the core loop of practical data work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INSY6500-Py4EDA",
   "language": "python",
   "name": "insy6500-py4eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
